{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Georgy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#загрузка необходимых библиотек \n",
    "\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import pprint\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import WordPunctTokenizer\n",
    "import docx2txt\n",
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models import Phrases\n",
    "from nltk.util import ngrams\n",
    "import rank_bm25\n",
    "\n",
    "#Определяем токенизатор, выделитель корней слов и стоп-слова для русского языка\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('russian')\n",
    "russian_stopwords = stopwords.words(\"russian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "#загружаем файл в формате docx, разделяем его на отдельные параграфы\\абзацы и удаляем пустые абзацы. Выводит массив, содержащий \n",
    "#параграфы с формате строк\n",
    "\n",
    "    doc = docx2txt.process(filename)\n",
    "    lines = doc.split('\\n')\n",
    "    lines = [line for line in lines if line != '']\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigrams(corpus):\n",
    "#Функция для построения биграмм слов в документах. На выходе: массив из подмассивов, в каждом подмассиве список биграмм из данного параграфа\n",
    "    corpus_2grams = []\n",
    "    for doc in corpus:\n",
    "        doc_2grams = list(ngrams(doc,2))\n",
    "        doc_2grams = ['_'.join(list(bigram)) for bigram in doc_2grams]\n",
    "        corpus_2grams.append(doc_2grams)\n",
    "    return corpus_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(lines, bigrams=False):\n",
    "\n",
    "#предобрабатываем массив параграфов: токенизируем, удаляем стоп-слова, пустые строки, знаки препинания, удаляем окончания слов.\n",
    "#на выходе: массив из под-массивов. В одном под-массиве содержатся отдельные слова параграфа в формате строки\n",
    "#если bigrams=True, функция выдаёт массив подмассивов, где в одном подмассиве находятся биграммы слов одного параграфа\n",
    "    corp = []\n",
    "    for parag in lines:\n",
    "        paragraph = []\n",
    "        parag = tokenizer.tokenize(parag)\n",
    "        for word in parag:\n",
    "            if word not in russian_stopwords and word != ' ' and word.strip() not in punctuation and word.strip()[-1] not in punctuation:\n",
    "                word = word.lower()\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                paragraph.append(stemmed_word)\n",
    "\n",
    "        corp.append(paragraph)\n",
    "    if bigrams:\n",
    "        corp_bigrams = build_bigrams(corp)\n",
    "        return corp_bigrams\n",
    "    else:\n",
    "        return corp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query, bigrams=False):\n",
    "#Предобрабатываем запрос по тому же шаблону, что и для целого документа. \n",
    "#На вход: пользовательский запрос в формате строки; на выход: Массив из предобработанных слов запроса\n",
    "#Если bigrams=True, то функция выдаёт биграммы слов в запросе\n",
    "    query = ' '.join(tokenizer.tokenize(query))\n",
    "    query_text = []\n",
    "    for word in query.split():\n",
    "        if word not in russian_stopwords and word != ' ' and word.strip() not in punctuation and word.strip()[-1] not in punctuation:\n",
    "            word = word.lower()\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            query_text.append(stemmed_word)\n",
    "    if bigrams:\n",
    "        query_bigrams = list(ngrams(query_text,2))\n",
    "        query_bigrams = ['_'.join(list(bigram)) for bigram in query_bigrams]\n",
    "        return query_bigrams\n",
    "    \n",
    "    else:\n",
    "        return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_or_lsi(corpus, method='tfidf'):\n",
    "    \n",
    "# построение модели для ранжирования документов. На вход: корпус текстов и метод (\"tfidf\" или \"lsi\"). На выход кортеж: (словарь\n",
    "# терминов в корпусе текстов, оцененная модель и матрица сходств слов) \n",
    "\n",
    "    dictionary = Dictionary(corpus)\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    model_tfidf = TfidfModel(corpus_bow)\n",
    "    corpus_tfidf = [model_tfidf[doc] for doc in corpus_bow]\n",
    "    simil_tfidf = MatrixSimilarity(corpus_tfidf)\n",
    "    if method == 'tfidf':\n",
    "        \n",
    "        return dictionary, model_tfidf, simil_tfidf\n",
    "    \n",
    "    elif method == 'lsi':\n",
    "        \n",
    "        model_lsi = LsiModel(corpus_tfidf,  id2word=dictionary, num_topics=50)\n",
    "        corpus_lsi = [model_lsi[doc] for doc in corpus_bow]\n",
    "        simil_lsi = MatrixSimilarity(corpus_lsi)\n",
    "        \n",
    "        return dictionary, model_lsi, simil_lsi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_tfidf(query, dictionary, model, similarity, ntop=6):\n",
    "\n",
    "#Находим топ N документов, наиболее близких запросу по критерию cosine similarity. \n",
    "#На вход: пользовательский запрос в формате сторки, словарь слов корпуса, модель tfidf и матрица сходств слов (берется из функии build_tfidf_or_lsi)\n",
    "#На выходе: Отсортированный массив из кортежей с (номер параграфа, значение cosine similarity)\n",
    "    \n",
    "    query_corp = dictionary.doc2bow(preprocess_query(query))\n",
    "    query_tfidf = model[query_corp]\n",
    "    query_simil = enumerate(similarity[query_tfidf])\n",
    "    query_top_docs = sorted(query_simil, key=lambda k: -k[1])\n",
    "    if len(query_top_docs) > ntop: \n",
    "        query_top_docs = query_top_docs[:ntop]\n",
    "    else:\n",
    "        query_top_docs = query_top_docs\n",
    "    \n",
    "    top_docs_indices = [elem[0] for elem in query_top_docs]\n",
    "    return top_docs_indices \n",
    "    \n",
    "\n",
    "def top_docs_lsi(query, dictionary, model, similarity, ntop=6):\n",
    "    \n",
    "#Находим топ N документов, наиболее близких запросу по критерию cosine similarity. \n",
    "#На вход: пользовательский запрос в формате сторки, словарь слов корпуса, модель lsi и матрица сходств слов (берется из функии build_tfidf_or_lsi)\n",
    "#На выходе: Отсортированный массив из кортежей с (номер параграфа, значение cosine similarity)\n",
    "\n",
    "    query_corp = dictionary.doc2bow(preprocess_query(query))\n",
    "    query_lsi = model[query_corp]\n",
    "    query_simil = enumerate(similarity[query_lsi])\n",
    "    query_top_docs = sorted(query_simil, key=lambda k: -k[1])\n",
    "    if len(query_top_docs) > ntop: \n",
    "        query_top_docs = query_top_docs[:ntop]\n",
    "    else:\n",
    "        query_top_docs = query_top_docs\n",
    "    top_docs_indices = [elem[0] for elem in query_top_docs]\n",
    "#Вместо этого: нужно выводить номера и названия глав, статей и пунктов, как они указаны в документах - (а также сам текст пунктов?)\n",
    "    return top_docs_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_bm25okapi(query, corp, ntop=6):\n",
    "    bm25okapi = rank_bm25.BM25Okapi(corp)\n",
    "    top_docs_indices = np.argsort((bm25okapi.get_scores(preprocess_query(query))))[::-1][:ntop]\n",
    "    top_docs_indices = list(top_docs_indices)\n",
    "    return top_docs_indices\n",
    "\n",
    "\n",
    "\n",
    "## На всякий случай: в rank-bm25 есть ещё BM25L и BM25+. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('fz152.docx')\n",
    "corp = preprocess_corpus(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, model_tfidf, simil_tfidf = build_tfidf_or_lsi(corp)\n",
    "dictionary, model_lsi, simil_lsi = build_tfidf_or_lsi(corp, method='lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_passages_from_doc(query, corp, doc, method='tfidf'):\n",
    "    \n",
    "##Вывод индекса и текста для топ-N пунктов из документа по критерию релевантности запросу. \n",
    "##На вход: текстовый запрос в формате строки, документ до предобработки и метод ранжирования пунктов документа\n",
    "#На выход: индекс и текст пунктов из документа, разделенных пустой строкой\n",
    "    if method == 'tfidf':\n",
    "        index_string = top_docs_tfidf(query, dictionary, model_tfidf, simil_tfidf)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "    elif method == 'lsi':\n",
    "        index_string = top_docs_lsi(query, dictionary, model_lsi, simil_lsi)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "    elif method == 'bm25okapi':\n",
    "        index_string = top_docs_bm25okapi(query, corp=corp)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('Неправильное значение method. Method может принимать значения \\'tfidf\\', \\'lsi\\' или \\'bm25okapi\\'. ')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.filter_n_most_frequent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('27: При обработке персональных данных должны быть обеспечены точность '\n",
      " 'персональных данных, их достаточность, а в необходимых случаях и '\n",
      " 'актуальность по отношению к целям обработки персональных данных. Оператор '\n",
      " 'должен принимать необходимые меры либо обеспечивать их принятие по удалению '\n",
      " 'или уточнению неполных или неточных данных.')\n",
      "\n",
      "\n",
      "('176: В случае подтверждения факта неточности персональных данных оператор на '\n",
      " 'основании сведений, представленных субъектом персональных данных или его '\n",
      " 'представителем либо уполномоченным органом по защите прав субъектов '\n",
      " 'персональных данных, или иных необходимых документов обязан уточнить '\n",
      " 'персональные данные либо обеспечить их уточнение (если обработка '\n",
      " 'персональных данных осуществляется другим лицом, действующим по поручению '\n",
      " 'оператора) в течение семи рабочих дней со дня представления таких сведений и '\n",
      " 'снять блокирование персональных данных.')\n",
      "\n",
      "\n",
      "('98: Субъект персональных данных имеет право на получение сведений, указанных '\n",
      " 'в части 7 настоящей статьи, за исключением случаев, предусмотренных частью 8 '\n",
      " 'настоящей статьи. Субъект персональных данных вправе требовать от оператора '\n",
      " 'уточнения его персональных данных, их блокирования или уничтожения в случае, '\n",
      " 'если персональные данные являются неполными, устаревшими, неточными, '\n",
      " 'незаконно полученными или не являются необходимыми для заявленной цели '\n",
      " 'обработки, а также принимать предусмотренные законом меры по защите своих '\n",
      " 'прав.')\n",
      "\n",
      "\n",
      "('173: Оператор обязан предоставить безвозмездно субъекту персональных данных '\n",
      " 'или его представителю возможность ознакомления с персональными данными, '\n",
      " 'относящимися к этому субъекту персональных данных. В срок, не превышающий '\n",
      " 'семи рабочих дней со дня предоставления субъектом персональных данных или '\n",
      " 'его представителем сведений, подтверждающих, что персональные данные '\n",
      " 'являются неполными, неточными или неактуальными, оператор обязан внести в '\n",
      " 'них необходимые изменения. В срок, не превышающий семи рабочих дней со дня '\n",
      " 'представления субъектом персональных данных или его представителем сведений, '\n",
      " 'подтверждающих, что такие персональные данные являются незаконно полученными '\n",
      " 'или не являются необходимыми для заявленной цели обработки, оператор обязан '\n",
      " 'уничтожить такие персональные данные. Оператор обязан уведомить субъекта '\n",
      " 'персональных данных или его представителя о внесенных изменениях и '\n",
      " 'предпринятых мерах и принять разумные меры для уведомления третьих лиц, '\n",
      " 'которым персональные данные этого субъекта были переданы.')\n",
      "\n",
      "\n",
      "('175: В случае выявления неправомерной обработки персональных данных при '\n",
      " 'обращении субъекта персональных данных или его представителя либо по запросу '\n",
      " 'субъекта персональных данных или его представителя либо уполномоченного '\n",
      " 'органа по защите прав субъектов персональных данных оператор обязан '\n",
      " 'осуществить блокирование неправомерно обрабатываемых персональных данных, '\n",
      " 'относящихся к этому субъекту персональных данных, или обеспечить их '\n",
      " 'блокирование (если обработка персональных данных осуществляется другим '\n",
      " 'лицом, действующим по поручению оператора) с момента такого обращения или '\n",
      " 'получения указанного запроса на период проверки. В случае выявления неточных '\n",
      " 'персональных данных при обращении субъекта персональных данных или его '\n",
      " 'представителя либо по их запросу или по запросу уполномоченного органа по '\n",
      " 'защите прав субъектов персональных данных оператор обязан осуществить '\n",
      " 'блокирование персональных данных, относящихся к этому субъекту персональных '\n",
      " 'данных, или обеспечить их блокирование (если обработка персональных данных '\n",
      " 'осуществляется другим лицом, действующим по поручению оператора) с момента '\n",
      " 'такого обращения или получения указанного запроса на период проверки, если '\n",
      " 'блокирование персональных данных не нарушает права и законные интересы '\n",
      " 'субъекта персональных данных или третьих лиц.')\n",
      "\n",
      "\n",
      "('30: обработка персональных данных осуществляется с согласия субъекта '\n",
      " 'персональных данных на обработку его персональных данных;')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_passages_from_doc('Что делать, если обнаружена неточность или неполнота в персональных данных?', doc, method='bm25okapi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_list = ['Когда субъект может отозвать своё согласие на обработку персональных данных?',\n",
    "                'Срок ответа обоснование отказа в обработке персональных данных', \n",
    "               'Когда запрещена трансграничная передача персональных данных?',\n",
    "                'Что должно включаться в согласие на обработку персональных данных?', \n",
    "                'Кто ответственен за защиту персональных данных?', \n",
    "                'Что делать, если обнаружена неточность или неполнота в персональных данных?',\n",
    "                'Можно ли использовать персональные данные в собственных целях?',\n",
    "                'Что делать в случае, когда субъект персональных данных недееспособен?',\n",
    "                'Данные о судимости субъекта персональных данных', \n",
    "                'Может ли осуществляться обработка биометрических данных без согласия субъекта?'\n",
    "                \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
