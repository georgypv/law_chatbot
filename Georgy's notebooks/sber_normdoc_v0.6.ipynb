{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Georgy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#загрузка необходимых библиотек \n",
    "\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import pprint\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import WordPunctTokenizer\n",
    "import docx2txt\n",
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models import Phrases\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.util import ngrams\n",
    "import rank_bm25\n",
    "\n",
    "#Определяем токенизатор, выделитель корней слов и стоп-слова для русского языка\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('russian')\n",
    "russian_stopwords = stopwords.words(\"russian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предварительно создаём словарь для замены жаргонных выражений и сокращений на выражения, встречающиеся в законах\n",
    "\n",
    "syn_ = [['пдн', 'персданные', \"пд\"],\n",
    "[\"физлицо\", \"фл\", \"физик\"],\n",
    "[\"юрлицо\", \"юл\", \"юрик\"],\n",
    "[\"россия\", \"росия\", 'рф'],\n",
    "['биометрика', \"биометрия\", \"биоданные\"],\n",
    "[\"цб\", \"центробанк\", \"центрбанк\", \"цб рф\"]]\n",
    "\n",
    "\n",
    "map_to_ = [['персональные', 'данные'], ['физическое', 'лицо'], ['юридическое', 'лицо'], ['российская', 'федерация'],\n",
    "           ['биометрические', 'данные'], ['центральный', 'банк']]\n",
    "#предварительная токенизация выражений, которые должны входить в словарь\n",
    "map_to =  []\n",
    "for s in map_to_:\n",
    "    temp_list = []\n",
    "    for word in s:\n",
    "        temp_list.append(stemmer.stem(word))\n",
    "    map_to.append(temp_list)\n",
    "    \n",
    "syn = []\n",
    "for s_list in syn_:\n",
    "    temp_list = []\n",
    "    for s in s_list:\n",
    "        s = stemmer.stem(s)\n",
    "        temp_list.append(s)\n",
    "    syn.append(temp_list)\n",
    "#Вот сам словарь; syn_map = {'физлиц': ['физическ', 'лиц'], ...}    \n",
    "syn_map = {}\n",
    "for idx,li in enumerate(syn):\n",
    "    for elem in li:\n",
    "        syn_map[elem] = map_to[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "#загружаем файл в формате docx, разделяем его на отдельные параграфы\\абзацы и удаляем пустые абзацы. Выводит массив, содержащий \n",
    "#параграфы с формате строк\n",
    "\n",
    "    doc = docx2txt.process(filename)\n",
    "    lines = doc.split('\\n')\n",
    "    lines = [line for line in lines if line != '']\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigrams(corpus):\n",
    "#Функция для построения биграмм слов в документах. На выходе: массив из подмассивов, в каждом подмассиве список биграмм из данного параграфа\n",
    "    corpus_2grams = []\n",
    "    for doc in corpus:\n",
    "        doc_2grams = list(ngrams(doc,2))\n",
    "        doc_2grams = ['_'.join(list(bigram)) for bigram in doc_2grams]\n",
    "        corpus_2grams.append(doc_2grams)\n",
    "    return corpus_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(lines, bigrams=False):\n",
    "\n",
    "#предобрабатываем массив параграфов: токенизируем, удаляем стоп-слова, пустые строки, знаки препинания, удаляем окончания слов.\n",
    "#на выходе: массив из под-массивов. В одном под-массиве содержатся отдельные слова параграфа в формате строки\n",
    "#если bigrams=True, функция выдаёт массив подмассивов, где в одном подмассиве находятся биграммы слов одного параграфа\n",
    "    corp = []\n",
    "    for parag in lines:\n",
    "        paragraph = []\n",
    "        parag = tokenizer.tokenize(parag)\n",
    "        for word in parag:\n",
    "            if word not in russian_stopwords and word != ' ' and word.strip() not in punctuation and word.strip()[-1] not in punctuation:\n",
    "                word = word.lower()\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                paragraph.append(stemmed_word)\n",
    "\n",
    "        corp.append(paragraph)\n",
    "    if bigrams:\n",
    "        corp_bigrams = build_bigrams(corp)\n",
    "        return corp_bigrams\n",
    "    else:\n",
    "        return corp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query, bigrams=False):\n",
    "#Предобрабатываем запрос по тому же шаблону, что и для целого документа. \n",
    "#На вход: пользовательский запрос в формате строки; на выход: Массив из предобработанных слов запроса\n",
    "#Если bigrams=True, то функция выдаёт биграммы слов в запросе\n",
    "    query = ' '.join(tokenizer.tokenize(query))\n",
    "    query_text = []\n",
    "    for word in query.split():\n",
    "        if word not in russian_stopwords and word != ' ' and word.strip() not in punctuation and word.strip()[-1] not in punctuation:\n",
    "            word = word.lower()\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            #если выражение из запроса в словаре syn_map, то оно заменяется на выражение из закона\n",
    "            #пока может заменять только слова, а не словосочетания\n",
    "            if stemmed_word in syn_map.keys():\n",
    "                stemmed_word = syn_map[stemmed_word]\n",
    "                if isinstance(stemmed_word, list):\n",
    "                    for s_w in stemmed_word:\n",
    "                        query_text.append(s_w)\n",
    "            else:\n",
    "                query_text.append(stemmed_word)\n",
    "        \n",
    "    if bigrams:\n",
    "        query_bigrams = list(ngrams(query_text,2))\n",
    "        query_bigrams = ['_'.join(list(bigram)) for bigram in query_bigrams]\n",
    "        return query_bigrams\n",
    "    \n",
    "    else:\n",
    "        return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_or_lsi(corpus, method='tfidf'):\n",
    "    \n",
    "# построение модели для ранжирования документов. На вход: корпус текстов и метод (\"tfidf\" или \"lsi\"). На выход кортеж: (словарь\n",
    "# терминов в корпусе текстов, оцененная модель и матрица сходств слов) \n",
    "\n",
    "    dictionary = Dictionary(corpus)\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    model_tfidf = TfidfModel(corpus_bow)\n",
    "    corpus_tfidf = [model_tfidf[doc] for doc in corpus_bow]\n",
    "    simil_tfidf = MatrixSimilarity(corpus_tfidf)\n",
    "    if method == 'tfidf':\n",
    "        \n",
    "        return dictionary, model_tfidf, simil_tfidf\n",
    "    \n",
    "    elif method == 'lsi':\n",
    "        \n",
    "        model_lsi = LsiModel(corpus_tfidf,  id2word=dictionary, num_topics=50)\n",
    "        corpus_lsi = [model_lsi[doc] for doc in corpus_bow]\n",
    "        simil_lsi = MatrixSimilarity(corpus_lsi)\n",
    "        \n",
    "        return dictionary, model_lsi, simil_lsi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_tfidf(query, dictionary, model, similarity, ntop=6):\n",
    "\n",
    "#Находим топ N документов, наиболее близких запросу по критерию cosine similarity. \n",
    "#На вход: пользовательский запрос в формате сторки, словарь слов корпуса, модель tfidf и матрица сходств слов (берется из функии build_tfidf_or_lsi)\n",
    "#На выходе: Отсортированный массив из кортежей с (номер параграфа, значение cosine similarity)\n",
    "    \n",
    "    query_corp = dictionary.doc2bow(preprocess_query(query))\n",
    "    query_tfidf = model[query_corp]\n",
    "    query_simil = enumerate(similarity[query_tfidf])\n",
    "    query_top_docs = sorted(query_simil, key=lambda k: -k[1])\n",
    "    if len(query_top_docs) > ntop: \n",
    "        query_top_docs = query_top_docs[:ntop]\n",
    "    else:\n",
    "        query_top_docs = query_top_docs\n",
    "    \n",
    "    top_docs_indices = [elem[0] for elem in query_top_docs]\n",
    "    return top_docs_indices \n",
    "    \n",
    "\n",
    "def top_docs_lsi(query, dictionary, model, similarity, ntop=6):\n",
    "    \n",
    "#Находим топ N документов, наиболее близких запросу по критерию cosine similarity. \n",
    "#На вход: пользовательский запрос в формате сторки, словарь слов корпуса, модель lsi и матрица сходств слов (берется из функии build_tfidf_or_lsi)\n",
    "#На выходе: Отсортированный массив из кортежей с (номер параграфа, значение cosine similarity)\n",
    "\n",
    "    query_corp = dictionary.doc2bow(preprocess_query(query))\n",
    "    query_lsi = model[query_corp]\n",
    "    query_simil = enumerate(similarity[query_lsi])\n",
    "    query_top_docs = sorted(query_simil, key=lambda k: -k[1])\n",
    "    if len(query_top_docs) > ntop: \n",
    "        query_top_docs = query_top_docs[:ntop]\n",
    "    else:\n",
    "        query_top_docs = query_top_docs\n",
    "    top_docs_indices = [elem[0] for elem in query_top_docs]\n",
    "#Вместо этого: нужно выводить номера и названия глав, статей и пунктов, как они указаны в документах - (а также сам текст пунктов?)\n",
    "    return top_docs_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_bm25okapi(query, corp, ntop):\n",
    "    bm25okapi = rank_bm25.BM25Okapi(corp)\n",
    "    top_docs_indices = np.argsort((bm25okapi.get_scores(preprocess_query(query))))[::-1][:ntop]\n",
    "    top_docs_indices = list(top_docs_indices)\n",
    "    return top_docs_indices\n",
    "\n",
    "\n",
    "\n",
    "## На всякий случай: в rank-bm25 есть ещё BM25L и BM25+. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc2vec(corp, min_count=1, vector_size=300, window=10, epochs=30):\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
    "    corpus = [gensim.models.doc2vec.TaggedDocument(d, [idx]) for idx, d in enumerate(corp)]\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, epochs=model.epochs, total_examples=model.corpus_count)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vec(corp, min_count=1, vector_size=300, window=10, iter=30):\n",
    "    model = Word2Vec(corp, min_count=min_count, size=vector_size, window=window, iter=30).wv\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Word2Vec(corp, min_count=1, size=300, window=10, iter=30).wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_doc2vec(query, doc_indices, model=doc2vec_model, ntop=5):\n",
    "    \n",
    "    vector_query = model.infer_vector(preprocess_query(query))\n",
    "    corpus = [gensim.models.doc2vec.TaggedDocument(d, [idx]) for idx, d in enumerate(corp)]\n",
    "    vectors = [model.infer_vector(corpus[doc_index].words) for doc_index in doc_indices]\n",
    "    cos_sim = [cosine_similarity(vector_query.reshape(1,-1), vector.reshape(1,-1)) for vector in vectors]\n",
    "    \n",
    "    sorted_tuples = sorted(zip(doc_indices, cos_sim), key= lambda k: k[1], reverse=True)\n",
    "    top_docs = [tup[0] for tup in sorted_tuples]\n",
    "    if len(top_docs) > ntop:\n",
    "        top_docs = top_docs[:ntop]\n",
    "        return top_docs\n",
    "    else:\n",
    "        top_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrase_word2vec(phrase, model=word2vec_model):\n",
    "    phrase_vector = []\n",
    "    for word in phrase:\n",
    "        if word in word2vec_model.vocab.keys():\n",
    "            vec_word = word2vec_model.get_vector(word)\n",
    "            phrase_vector.append(vec_word)\n",
    "        else:\n",
    "            continue\n",
    "    if len(phrase_vector) > 0:\n",
    "        phrase_vector = np.mean(phrase_vector, axis=0)\n",
    "    else:\n",
    "        phrase_vector = np.zeros([word2vec_model.vector_size], dtype='float32')\n",
    "    return phrase_vector\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_word2vec(query, corp, doc_indices, wmd=True, ntop=5):\n",
    "    query_vector = build_phrase_word2vec(preprocess_query(query))\n",
    "    doc_vectors = []\n",
    "    for idx in doc_indices:\n",
    "        doc_vector = build_phrase_word2vec(corp[idx])\n",
    "        doc_vectors.append(doc_vector)\n",
    "    if wmd:\n",
    "        \n",
    "        word2vec_model.init_sims(replace=True)\n",
    "        wm_dist = [word2vec_model.wmdistance(preprocess_query(query), corp[doc_idx]) for doc_idx in doc_indices]\n",
    "        sorted_tuples = sorted(zip(doc_indices, wm_dist), key= lambda k: k[1], reverse=True)\n",
    "        top_docs = [tup[0] for tup in sorted_tuples]\n",
    "    \n",
    "    else:\n",
    "        cos_sim = [cosine_similarity(query_vector.reshape(1, -1), doc_vector.reshape(1,-1)) for doc_vector in doc_vectors]\n",
    "        sorted_tuples = sorted(zip(doc_indices, cos_sim), key= lambda k: k[1], reverse=True)\n",
    "        top_docs = [tup[0] for tup in sorted_tuples]\n",
    "    \n",
    "    \n",
    "    if len(top_docs) > ntop:\n",
    "        top_docs = top_docs[:ntop]\n",
    "        return top_docs\n",
    "    else:\n",
    "        top_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('laws.docx')\n",
    "corp = preprocess_corpus(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc2vec_model = build_doc2vec(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = build_word2vec(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, model_tfidf, simil_tfidf = build_tfidf_or_lsi(corp)\n",
    "dictionary, model_lsi, simil_lsi = build_tfidf_or_lsi(corp, method='lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_passages_from_doc(query, corp, doc, method='bm25okapi'):\n",
    "    \n",
    "##Вывод индекса и текста для топ-N пунктов из документа по критерию релевантности запросу. \n",
    "##На вход: текстовый запрос в формате строки, документ до предобработки и метод ранжирования пунктов документа\n",
    "#На выход: индекс и текст пунктов из документа, разделенных пустой строкой\n",
    "    if method == 'tfidf':\n",
    "        index_string = top_docs_tfidf(query, dictionary, model_tfidf, simil_tfidf)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "    elif method == 'lsi':\n",
    "        index_string = top_docs_lsi(query, dictionary, model_lsi, simil_lsi)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "    elif method == 'bm25okapi':\n",
    "        index_string = top_docs_bm25okapi(query, corp=corp, ntop=5)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "    elif method == 'doc2vec':\n",
    "        index_string = top_docs_bm25okapi(query, corp=corp, ntop=10)\n",
    "        for idx in top_docs_doc2vec(query, index_string, model=doc2vec_model, ntop=5):\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "    elif method == 'word2vec':\n",
    "        index_string = top_docs_bm25okapi(query, corp=corp, ntop=10)\n",
    "        for idx in top_docs_word2vec(query, corp, index_string, ntop=5):\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "\n",
    "            \n",
    "    else:\n",
    "        print('Неправильное значение method. Method может принимать значения \\'tfidf\\', \\'lsi\\',\\'bm25okapi\\', \\'doc2vec\\' или \\'word2vec\\'. ')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.filter_n_most_frequent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('995: 5.2. В случае, если представление информации источником формирования '\n",
      " 'кредитных историй, указанным в части 5.1 настоящей статьи, осуществляется в '\n",
      " 'отношении более чем четырех субъектов кредитной истории в течение одного '\n",
      " 'года, источник формирования кредитных историй обязан представлять информацию '\n",
      " 'в форме электронного документа.')\n",
      "\n",
      "\n",
      "('1000: 5.7. Источник формирования кредитной истории обязан представлять в '\n",
      " 'бюро кредитных историй информацию, определенную частью 10 статьи 4 '\n",
      " 'настоящего Федерального закона, в течение всего срока хранения источником '\n",
      " 'формирования кредитной истории информации, определенной статьей 4 настоящего '\n",
      " 'Федерального закона.')\n",
      "\n",
      "\n",
      "('993: 5. Источники формирования кредитной истории представляют информацию в '\n",
      " 'бюро кредитных историй в срок, предусмотренный договором о предоставлении '\n",
      " 'информации, но не позднее пяти рабочих дней со дня совершения действия '\n",
      " '(наступления события), информация о котором входит в состав кредитной '\n",
      " 'истории в соответствии с настоящим Федеральным законом, либо со дня, когда '\n",
      " 'источнику формирования кредитной истории стало известно о совершении такого '\n",
      " 'действия (наступлении такого события). Источники формирования кредитной '\n",
      " 'истории (за исключением источников, указанных в части 5.1 настоящей статьи) '\n",
      " 'представляют информацию в бюро кредитных историй в форме электронного '\n",
      " 'документа.')\n",
      "\n",
      "\n",
      "('1068: 2. Бюро кредитных историй имеет право приостановить получение '\n",
      " 'информации от источников формирования кредитных историй и (или) '\n",
      " 'предоставление кредитных отчетов на период проведения ликвидационных или '\n",
      " 'реорганизационных процедур. В этом случае оно в течение трех рабочих дней со '\n",
      " 'дня принятия решения о приостановлении получения информации от источников '\n",
      " 'формирования кредитных историй и (или) предоставления кредитных отчетов '\n",
      " 'обязано уведомить об этом источники формирования кредитных историй, '\n",
      " 'предоставляющие ему такую информацию, а также разместить соответствующую '\n",
      " 'информацию в общероссийском периодическом печатном издании и местном '\n",
      " 'периодическом печатном издании по местонахождению ликвидируемого '\n",
      " '(реорганизуемого) бюро кредитных историй.')\n",
      "\n",
      "\n",
      "('1053: 4.2. В случае, если в течение установленного срока бюро кредитных '\n",
      " 'историй не получило ответ на запрос, указанный в части 4.1 настоящей статьи, '\n",
      " 'от источника формирования кредитной истории в связи с заявлением субъекта '\n",
      " 'кредитной истории о внесении изменений в его кредитную историю, источник '\n",
      " 'формирования кредитной истории несет ответственность, установленную '\n",
      " 'законодательством Российской Федерации.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_passages_from_doc('источник формирования кредитной истории что это', corp, doc, method='word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_list = ['Когда субъект может отозвать своё согласие на обработку персональных данных?',\n",
    "                'Срок ответа обоснование отказа в обработке персональных данных', \n",
    "               'Когда запрещена трансграничная передача персональных данных?',\n",
    "                'Что должно включаться в согласие на обработку персональных данных?', \n",
    "                'Кто ответственен за защиту персональных данных?', \n",
    "                'Что делать, если обнаружена неточность или неполнота в персональных данных?',\n",
    "                'Можно ли использовать персональные данные в собственных целях?',\n",
    "                'Что делать в случае, когда субъект персональных данных недееспособен?',\n",
    "                'Данные о судимости субъекта персональных данных', \n",
    "                'Может ли осуществляться обработка биометрических данных без согласия субъекта?'\n",
    "                \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws = [\"86-ФЗ О Центральном Банке\", \n",
    "        \" 353-ФЗ О потребительском кредите (займе)\",\n",
    "        \"218-ФЗ О кредитных историях\", \n",
    "        \"126-ФЗ О связи\",\n",
    "       \"149-ФЗ Об информации, информационных технологиях и о защите информации\", \n",
    "        \"230-ФЗ О защите прав и законных интересов\",\n",
    "        #\"395-ФЗ О банках и банковской деятельности\", \n",
    "        \"Постановление правительства N 687 Об утверждении Положения об особенностях обработки...\",\n",
    "        \"Постановление правительства N 512 Об утверждении требований к материальным носителям\",\n",
    "        \"499-П Об идентификации кредитными организациями клиентов, представителей клиента...\",\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = np.array([2, 3, 5]).reshape(1,-1)\n",
    "vec2 = np.array([-0.2, 0.4, 5]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83373424]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
