{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Georgy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#загрузка необходимых библиотек \n",
    "\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import pprint\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import WordPunctTokenizer\n",
    "import docx2txt\n",
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models import Phrases\n",
    "from nltk.util import ngrams\n",
    "import rank_bm25\n",
    "\n",
    "#Определяем токенизатор, выделитель корней слов и стоп-слова для русского языка\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('russian')\n",
    "russian_stopwords = stopwords.words(\"russian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предварительно создаём словарь для замены жаргонных выражений и сокращений на выражения, встречающиеся в законах\n",
    "\n",
    "syn_ = [['пдн', 'персданные', \"пд\"],\n",
    "[\"физлицо\", \"фл\", \"физик\"],\n",
    "[\"юрлицо\", \"юл\", \"юрик\"],\n",
    "[\"россия\", \"росия\", 'рф'],\n",
    "['биометрика', \"биометрия\", \"биоданные\"],\n",
    "[\"цб\", \"центробанк\", \"центрбанк\", \"цб рф\"]]\n",
    "\n",
    "\n",
    "map_to_ = [['персональные', 'данные'], ['физическое', 'лицо'], ['юридическое', 'лицо'], ['российская', 'федерация'],\n",
    "           ['биометрические', 'данные'], ['центральный', 'банк']]\n",
    "#предварительная токенизация выражений, которые должны входить в словарь\n",
    "map_to =  []\n",
    "for s in map_to_:\n",
    "    temp_list = []\n",
    "    for word in s:\n",
    "        temp_list.append(stemmer.stem(word))\n",
    "    map_to.append(temp_list)\n",
    "    \n",
    "syn = []\n",
    "for s_list in syn_:\n",
    "    temp_list = []\n",
    "    for s in s_list:\n",
    "        s = stemmer.stem(s)\n",
    "        temp_list.append(s)\n",
    "    syn.append(temp_list)\n",
    "#Вот сам словарь; syn_map = {'физлиц': ['физическ', 'лиц'], ...}    \n",
    "syn_map = {}\n",
    "for idx,li in enumerate(syn):\n",
    "    for elem in li:\n",
    "        syn_map[elem] = map_to[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "#загружаем файл в формате docx, разделяем его на отдельные параграфы\\абзацы и удаляем пустые абзацы. Выводит массив, содержащий \n",
    "#параграфы с формате строк\n",
    "\n",
    "    doc = docx2txt.process(filename)\n",
    "    lines = doc.split('\\n')\n",
    "    lines = [line for line in lines if line != '']\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigrams(corpus):\n",
    "#Функция для построения биграмм слов в документах. На выходе: массив из подмассивов, в каждом подмассиве список биграмм из данного параграфа\n",
    "    corpus_2grams = []\n",
    "    for doc in corpus:\n",
    "        doc_2grams = list(ngrams(doc,2))\n",
    "        doc_2grams = ['_'.join(list(bigram)) for bigram in doc_2grams]\n",
    "        corpus_2grams.append(doc_2grams)\n",
    "    return corpus_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(lines, bigrams=False):\n",
    "\n",
    "#предобрабатываем массив параграфов: токенизируем, удаляем стоп-слова, пустые строки, знаки препинания, удаляем окончания слов.\n",
    "#на выходе: массив из под-массивов. В одном под-массиве содержатся отдельные слова параграфа в формате строки\n",
    "#если bigrams=True, функция выдаёт массив подмассивов, где в одном подмассиве находятся биграммы слов одного параграфа\n",
    "    corp = []\n",
    "    for parag in lines:\n",
    "        paragraph = []\n",
    "        parag = tokenizer.tokenize(parag)\n",
    "        for word in parag:\n",
    "            if word not in russian_stopwords and word != ' ' and word.strip() not in punctuation and word.strip()[-1] not in punctuation:\n",
    "                word = word.lower()\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                paragraph.append(stemmed_word)\n",
    "\n",
    "        corp.append(paragraph)\n",
    "    if bigrams:\n",
    "        corp_bigrams = build_bigrams(corp)\n",
    "        return corp_bigrams\n",
    "    else:\n",
    "        return corp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query, bigrams=False):\n",
    "#Предобрабатываем запрос по тому же шаблону, что и для целого документа. \n",
    "#На вход: пользовательский запрос в формате строки; на выход: Массив из предобработанных слов запроса\n",
    "#Если bigrams=True, то функция выдаёт биграммы слов в запросе\n",
    "    query = ' '.join(tokenizer.tokenize(query))\n",
    "    query_text = []\n",
    "    for word in query.split():\n",
    "        if word not in russian_stopwords and word != ' ' and word.strip() not in punctuation and word.strip()[-1] not in punctuation:\n",
    "            word = word.lower()\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            #если выражение из запроса в словаре syn_map, то оно заменяется на выражение из закона\n",
    "            #пока может заменять только слова, а не словосочетания\n",
    "            if stemmed_word in syn_map.keys():\n",
    "                stemmed_word = syn_map[stemmed_word]\n",
    "                if isinstance(stemmed_word, list):\n",
    "                    for s_w in stemmed_word:\n",
    "                        query_text.append(s_w)\n",
    "            else:\n",
    "                query_text.append(stemmed_word)\n",
    "        \n",
    "    if bigrams:\n",
    "        query_bigrams = list(ngrams(query_text,2))\n",
    "        query_bigrams = ['_'.join(list(bigram)) for bigram in query_bigrams]\n",
    "        return query_bigrams\n",
    "    \n",
    "    else:\n",
    "        return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_or_lsi(corpus, method='tfidf'):\n",
    "    \n",
    "# построение модели для ранжирования документов. На вход: корпус текстов и метод (\"tfidf\" или \"lsi\"). На выход кортеж: (словарь\n",
    "# терминов в корпусе текстов, оцененная модель и матрица сходств слов) \n",
    "\n",
    "    dictionary = Dictionary(corpus)\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    model_tfidf = TfidfModel(corpus_bow)\n",
    "    corpus_tfidf = [model_tfidf[doc] for doc in corpus_bow]\n",
    "    simil_tfidf = MatrixSimilarity(corpus_tfidf)\n",
    "    if method == 'tfidf':\n",
    "        \n",
    "        return dictionary, model_tfidf, simil_tfidf\n",
    "    \n",
    "    elif method == 'lsi':\n",
    "        \n",
    "        model_lsi = LsiModel(corpus_tfidf,  id2word=dictionary, num_topics=50)\n",
    "        corpus_lsi = [model_lsi[doc] for doc in corpus_bow]\n",
    "        simil_lsi = MatrixSimilarity(corpus_lsi)\n",
    "        \n",
    "        return dictionary, model_lsi, simil_lsi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_tfidf(query, dictionary, model, similarity, ntop=6):\n",
    "\n",
    "#Находим топ N документов, наиболее близких запросу по критерию cosine similarity. \n",
    "#На вход: пользовательский запрос в формате сторки, словарь слов корпуса, модель tfidf и матрица сходств слов (берется из функии build_tfidf_or_lsi)\n",
    "#На выходе: Отсортированный массив из кортежей с (номер параграфа, значение cosine similarity)\n",
    "    \n",
    "    query_corp = dictionary.doc2bow(preprocess_query(query))\n",
    "    query_tfidf = model[query_corp]\n",
    "    query_simil = enumerate(similarity[query_tfidf])\n",
    "    query_top_docs = sorted(query_simil, key=lambda k: -k[1])\n",
    "    if len(query_top_docs) > ntop: \n",
    "        query_top_docs = query_top_docs[:ntop]\n",
    "    else:\n",
    "        query_top_docs = query_top_docs\n",
    "    \n",
    "    top_docs_indices = [elem[0] for elem in query_top_docs]\n",
    "    return top_docs_indices \n",
    "    \n",
    "\n",
    "def top_docs_lsi(query, dictionary, model, similarity, ntop=6):\n",
    "    \n",
    "#Находим топ N документов, наиболее близких запросу по критерию cosine similarity. \n",
    "#На вход: пользовательский запрос в формате сторки, словарь слов корпуса, модель lsi и матрица сходств слов (берется из функии build_tfidf_or_lsi)\n",
    "#На выходе: Отсортированный массив из кортежей с (номер параграфа, значение cosine similarity)\n",
    "\n",
    "    query_corp = dictionary.doc2bow(preprocess_query(query))\n",
    "    query_lsi = model[query_corp]\n",
    "    query_simil = enumerate(similarity[query_lsi])\n",
    "    query_top_docs = sorted(query_simil, key=lambda k: -k[1])\n",
    "    if len(query_top_docs) > ntop: \n",
    "        query_top_docs = query_top_docs[:ntop]\n",
    "    else:\n",
    "        query_top_docs = query_top_docs\n",
    "    top_docs_indices = [elem[0] for elem in query_top_docs]\n",
    "#Вместо этого: нужно выводить номера и названия глав, статей и пунктов, как они указаны в документах - (а также сам текст пунктов?)\n",
    "    return top_docs_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_docs_bm25okapi(query, corp, ntop=6):\n",
    "    bm25okapi = rank_bm25.BM25Okapi(corp)\n",
    "    top_docs_indices = np.argsort((bm25okapi.get_scores(preprocess_query(query))))[::-1][:ntop]\n",
    "    top_docs_indices = list(top_docs_indices)\n",
    "    return top_docs_indices\n",
    "\n",
    "\n",
    "\n",
    "## На всякий случай: в rank-bm25 есть ещё BM25L и BM25+. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc2vec(corp, min_count=1, vector_size=100, window=6, sample=1e-3, epochs=10):\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=vector_size, window=window, sample=sample, min_count=min_count, epochs=epochs)\n",
    "    corpus = [gensim.models.doc2vec.TaggedDocument(d, [idx]) for idx, d in enumerate(corp)]\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, epochs=model.epochs, total_examples=model.corpus_count)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('laws.docx')\n",
    "corp = preprocess_corpus(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, model_tfidf, simil_tfidf = build_tfidf_or_lsi(corp)\n",
    "dictionary, model_lsi, simil_lsi = build_tfidf_or_lsi(corp, method='lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-740cf29b2bb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc2vec_corp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_doc2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-170-ce6a76b86b8b>\u001b[0m in \u001b[0;36mbuild_doc2vec\u001b[1;34m(corp, min_count, vector_size, window, sample, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "doc2vec_corp = build_doc2vec(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_passages_from_doc(query, corp, doc, method='bm25okapi'):\n",
    "    \n",
    "##Вывод индекса и текста для топ-N пунктов из документа по критерию релевантности запросу. \n",
    "##На вход: текстовый запрос в формате строки, документ до предобработки и метод ранжирования пунктов документа\n",
    "#На выход: индекс и текст пунктов из документа, разделенных пустой строкой\n",
    "    if method == 'tfidf':\n",
    "        index_string = top_docs_tfidf(query, dictionary, model_tfidf, simil_tfidf)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "    elif method == 'lsi':\n",
    "        index_string = top_docs_lsi(query, dictionary, model_lsi, simil_lsi)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "    elif method == 'bm25okapi':\n",
    "        index_string = top_docs_bm25okapi(query, corp=corp)\n",
    "        for idx in index_string:\n",
    "            answer = str(idx) + ': ' + doc[idx]\n",
    "            pprint.pprint(answer)\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print('Неправильное значение method. Method может принимать значения \\'tfidf\\', \\'lsi\\' или \\'bm25okapi\\'. ')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.filter_n_most_frequent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2128: 12. Оператор вправе установить не противоречащие требованиям '\n",
      " 'законодательства Российской Федерации дополнительные требования к '\n",
      " 'технологиям хранения биометрических персональных данных вне информационных '\n",
      " 'систем персональных данных в зависимости от методов и способов защиты '\n",
      " 'биометрических персональных данных в информационных системах персональных '\n",
      " 'данных этого оператора.')\n",
      "\n",
      "\n",
      "('2119: 4. Материальный носитель должен обеспечивать: защиту от '\n",
      " 'несанкционированной повторной и дополнительной записи информации после ее '\n",
      " 'извлечения из информационной системы персональных данных; возможность '\n",
      " 'доступа к записанным на материальный носитель биометрическим персональным '\n",
      " 'данным, осуществляемого оператором и лицами, уполномоченными в соответствии '\n",
      " 'с законодательством Российской Федерации на работу с биометрическими '\n",
      " 'персональными данными (далее - уполномоченные лица); возможность '\n",
      " 'идентификации информационной системы персональных данных, в которую была '\n",
      " 'осуществлена запись биометрических персональных данных, а также оператора, '\n",
      " 'осуществившего такую запись; невозможность несанкционированного доступа к '\n",
      " 'биометрическим персональным данным, содержащимся на материальном носителе.')\n",
      "\n",
      "\n",
      "('1845: 10. Контроль и надзор за выполнением организационных и технических мер '\n",
      " 'по обеспечению безопасности персональных данных, установленных в '\n",
      " 'соответствии со статьей 19 Федерального закона от 27 июля 2006 года N 152-ФЗ '\n",
      " '\"О персональных данных\", при обработке персональных данных в единой '\n",
      " 'биометрической системе, за исключением контроля и надзора за выполнением '\n",
      " 'банками организационных и технических мер по обеспечению безопасности '\n",
      " 'персональных данных при использовании единой биометрической системы, '\n",
      " 'осуществляются федеральным органом исполнительной власти, уполномоченным в '\n",
      " 'области обеспечения безопасности, и федеральным органом исполнительной '\n",
      " 'власти, уполномоченным в области противодействия техническим разведкам и '\n",
      " 'технической защиты информации, в пределах их полномочий, установленных '\n",
      " 'законодательством Российской Федерации о персональных данных.')\n",
      "\n",
      "\n",
      "('1982: 1.1. Лица, виновные в нарушении требований статьи 14.1 настоящего '\n",
      " 'Федерального закона в части обработки, включая сбор и хранение, '\n",
      " 'биометрических персональных данных, несут административную, гражданскую и '\n",
      " 'уголовную ответственность в соответствии с законодательством Российской '\n",
      " 'Федерации.')\n",
      "\n",
      "\n",
      "('1849: 14. Центральный банк Российской Федерации совместно с оператором '\n",
      " 'единой биометрической системы определяет перечень угроз безопасности, '\n",
      " 'актуальных при обработке, включая сбор и хранение, биометрических '\n",
      " 'персональных данных, их проверке и передаче информации о степени их '\n",
      " 'соответствия предоставленным биометрическим персональным данным гражданина '\n",
      " 'Российской Федерации в государственных органах, банках и иных организациях, '\n",
      " 'указанных в абзаце первом части 1 настоящей статьи, в единой биометрической '\n",
      " 'системе с учетом оценки возможного вреда, проведенной в соответствии с '\n",
      " 'законодательством Российской Федерации о персональных данных, и '\n",
      " 'согласовывает указанный перечень с федеральным органом исполнительной '\n",
      " 'власти, уполномоченным в области обеспечения безопасности, и федеральным '\n",
      " 'органом исполнительной власти, уполномоченным в области противодействия '\n",
      " 'техническим разведкам и технической защиты информации.')\n",
      "\n",
      "\n",
      "('2108: При ведении журналов (реестров, книг), содержащих персональные данные, '\n",
      " 'необходимые для однократного пропуска субъекта персональных данных на '\n",
      " 'территорию, на которой находится оператор, или в иных аналогичных целях, '\n",
      " 'должны соблюдаться следующие условия: необходимость ведения такого журнала '\n",
      " '(реестра, книги) должна быть предусмотрена актом оператора, содержащим '\n",
      " 'сведения о цели обработки персональных данных, осуществляемой без '\n",
      " 'использования средств автоматизации, способы фиксации и состав информации, '\n",
      " 'запрашиваемой у субъектов персональных данных, перечень лиц (поименно или по '\n",
      " 'должностям), имеющих доступ к материальным носителям и ответственных за '\n",
      " 'ведение и сохранность журнала (реестра, книги), сроки обработки персональных '\n",
      " 'данных, а также сведения о порядке пропуска субъекта персональных данных на '\n",
      " 'территорию, на которой находится оператор, без подтверждения подлинности '\n",
      " 'персональных данных, сообщенных субъектом персональных данных; копирование '\n",
      " 'содержащейся в таких журналах (реестрах, книгах) информации не допускается; '\n",
      " 'персональные данные каждого субъекта персональных данных могут заноситься в '\n",
      " 'такой журнал (книгу, реестр) не более одного раза в каждом случае пропуска '\n",
      " 'субъекта персональных данных на территорию, на которой находится оператор.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_passages_from_doc('защита пд кто ответственен в рф', corp, doc, method='bm25okapi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_list = ['Когда субъект может отозвать своё согласие на обработку персональных данных?',\n",
    "                'Срок ответа обоснование отказа в обработке персональных данных', \n",
    "               'Когда запрещена трансграничная передача персональных данных?',\n",
    "                'Что должно включаться в согласие на обработку персональных данных?', \n",
    "                'Кто ответственен за защиту персональных данных?', \n",
    "                'Что делать, если обнаружена неточность или неполнота в персональных данных?',\n",
    "                'Можно ли использовать персональные данные в собственных целях?',\n",
    "                'Что делать в случае, когда субъект персональных данных недееспособен?',\n",
    "                'Данные о судимости субъекта персональных данных', \n",
    "                'Может ли осуществляться обработка биометрических данных без согласия субъекта?'\n",
    "                \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws = [\"86-ФЗ О Центральном Банке\", \n",
    "        \" 353-ФЗ О потребительском кредите (займе)\",\n",
    "        \"218-ФЗ О кредитных историях\", \n",
    "        \"126-ФЗ О связи\",\n",
    "       \"149-ФЗ Об информации, информационных технологиях и о защите информации\", \n",
    "        \"230-ФЗ О защите прав и законных интересов\",\n",
    "        #\"395-ФЗ О банках и банковской деятельности\", \n",
    "        \"Постановление правительства N 687 Об утверждении Положения об особенностях обработки...\",\n",
    "        \"Постановление правительства N 512 Об утверждении требований к материальным носителям\",\n",
    "        \"499-П Об идентификации кредитными организациями клиентов, представителей клиента...\",\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
